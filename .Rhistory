value = 5),
br(),
sliderInput("range1", "Range of word sizes",
min = 0.1, max = 5,
value = c(0.25,2.5)),
br(),
checkboxGroupInput("checkGroup",
label = h4("Parts of speech for cooccurance plot"),
choices = list("Adjective" = "ADJ",
"Noun" = "NOUN",
"Proper noun" = "PROPN",
"Adverb" = "ADV",
"Verb" = "VERB"),
selected = c("ADJ", "NOUN")), #default selection
br(),
sliderInput("integer2", "Size of co-occurance plot",
min = 0, max = 10,
value = 5)
), #end of sidebarPanel
mainPanel(
#"Title for main panel",
tabsetPanel(type="tabs",
tabPanel("Read me",
h3('About this app'),
p("Need to write what this app does here")), #end of tabPanel
tabPanel("Annotated document",
h3('Annotated document'),
p('Table of the annotated document as a dataframe'),
dataTableOutput('mytable1'),
downloadButton("downloadText", "Download Annotated Text")
), #end of tabPanel
tabPanel("Wordclouds",
h3('Nouns'),
p('Here is a wordcloud of the nouns in the uploaded document'),
br(), plotOutput('nounwordcloud'),
br(), h3('Verbs'),
p('Here is a wordcloud of the verbs in the uploaded document'),
br(), plotOutput('verbwordcloud')), #end of tabPanel
tabPanel("Cooccurances",
h3('Co-occurances'),
p('Here is a coccurance plot of the selected parts of speech'),
br(), plotOutput("anndoccooc")) #end of tabPanel
) #end of tabsetPanel
) #end of mainPanel
) #end of sidebarLayout
) #end of fluidpage
knitr::opts_chunk$set(echo = TRUE)
if (!require(dplyr)) {install.packages("dplyr")}; library(dplyr)
if (!require(udpipe)) {install.packages("udpipe")}; library(udpipe)
if (!require(textrank)) {install.packages("textrank")}; library(textrank)
if (!require(lattice)) {install.packages("lattice")}; library(lattice)
if (!require(igraph)) {install.packages("igraph")}; library(igraph)
if (!require(ggraph)) {install.packages("ggraph")}; library(ggraph)
if (!require(ggplot2)) {install.packages("ggplot2")}; library(ggplot2)
if (!require(wordcloud)) {install.packages("wordcloud")}; library(wordcloud)
if (!require(RColorBrewer)) {install.packages("RColorBrewer")}; library(RColorBrewer)
if (!require(stringr)) {install.packages("stringr")}; library(stringr)
if (!require(textreadr)) {install.packages("textreadr")}; library(textreadr)
if (!require(tools)) {install.packages("tools")}; library(tools)
if (!require(shiny)) {install.packages("shiny")}; library(shiny)
if (!require(shinyjs)) {install.packages("shinyjs")}; library(shinyjs)
jscode <- "shinyjs.closeWindow = function() { window.close(); }"
ui <- fluidPage(
#titlePanel("Natural Language Processing"),
sidebarLayout(
sidebarPanel(
fileInput("file1", "Upload a file (pdf, docx or txt)",
accept=c('.pdf', '.txt', '.docx'),
multiple=TRUE),
fileInput("file2", "Upload another file (pdf, docx or txt)",
accept=c('.pdf', '.txt', '.docx'),
multiple=TRUE),
radioButtons("model", label = h4("Download English model?"),
choices = list("Yes" = "Yes",
"No" = "No"),
selected = "No"), #default selection
br(),
radioButtons("radio", label = h4("Colors for the wordclouds"),
choices = list("Accent" = "Accent",
"Dark2" = "Dark2",
"Paired" = "Paired",
"Pastel1" = "Pastel1",
"Pastel2" = "Pastel2",
"Paired" = "Paired"),
selected = "Accent"), #default selection
br(),
sliderInput("integer1", "Number of words to plot",
min = 0, max = 100,
value = 50),
br(),
sliderInput("integer3", "Min freq of words to plot",
min = 1, max = 10,
value = 5),
br(),
sliderInput("range1", "Range of word sizes",
min = 0.1, max = 5,
value = c(0.25,2.5)),
br(),
checkboxGroupInput("checkGroup",
label = h4("Parts of speech for cooccurance plot"),
choices = list("Adjective" = "ADJ",
"Noun" = "NOUN",
"Proper noun" = "PROPN",
"Adverb" = "ADV",
"Verb" = "VERB"),
selected = c("ADJ", "NOUN")), #default selection
br(),
sliderInput("integer2", "Size of co-occurance plot",
min = 0, max = 10,
value = 5)
), #end of sidebarPanel
mainPanel(
#"Title for main panel",
tabsetPanel(type="tabs",
tabPanel("Read me",
h3('About this app'),
p("Need to write what this app does here")), #end of tabPanel
tabPanel("Annotated document",
h3('Annotated document'),
p('Table of the annotated document as a dataframe'),
dataTableOutput('mytable1'),
downloadButton("downloadText", "Download Annotated Text")
), #end of tabPanel
tabPanel("Wordclouds",
h3('Nouns'),
p('Here is a wordcloud of the nouns in the uploaded document'),
br(), plotOutput('nounwordcloud'),
br(), h3('Verbs'),
p('Here is a wordcloud of the verbs in the uploaded document'),
br(), plotOutput('verbwordcloud')), #end of tabPanel
tabPanel("Cooccurances",
h3('Co-occurances'),
p('Here is a coccurance plot of the selected parts of speech'),
br(), plotOutput("anndoccooc")) #end of tabPanel
) #end of tabsetPanel
) #end of mainPanel
) #end of sidebarLayout
) #end of fluidpage
server = function(input, output, session){
Dataset = reactive({
req(input$file1) #ensure file has been uploaded, if not, stop
inFile <- input$file1
ext = file_ext(sub("\\?.+", "", inFile$name))
if(is.null(inFile)) {return(NULL)}
else{
if(ext == 'txt') { #check if file extension is txt
text1 = readLines(inFile$datapath) #read the file
text2 = str_replace_all(text1, "<.*?>", "") # get rid of html junk
return(text2)}
else{
if(ext == 'pdf') { #check if file extension is pdf
text1 = pdf_text(inFile$datapath) #read the file
text2 = str_replace_all(text1, "<.*?>", "") # get rid of html junk
return(text2)}
else{
if(ext == 'docx') { #check if file extension is docx
text1 = read_docx(inFile$datapath) #read the file
text2 = str_replace_all(text1, "<.*?>", "") # get rid of html junk
return(text2)}}}
}
}) #end of Dataset
englishmodel = reactive({
model = input$model #download the english model? yes or no
if(model == "Yes") { #is user does not have the english model in current working directory
x = udpipe_download_model(language = "english") #download the english model
englishmodel = udpipe_load_model(x$file_model) #load the english model
}
else{ #if user already has the english model in current working directory
englishmodel = udpipe_load_model("english-ud-2.0-170801.udpipe") #load the english model
}
#to make the app download the english model and load it at clients end
return(englishmodel)
}) #end of english model
anntext = reactive({
anndoc = udpipe_annotate(englishmodel(), Dataset()) #tokenizes, tags and parses the text
anndoc = as.data.frame(anndoc) #converts the output into a data frame
anndoc = mutate(anndoc, sentence=NULL) #drop the sentence column from the data frame
head(anndoc, 100) #show only a hundred rows and store in a new variable
return(anndoc)
}) #end of ann.text
output$downloadText = downloadHandler(
# Tell the client browser what name to use when saving the file
filename = function() {
paste("annotated_text", ".csv", sep = "")
},
content = function(file){
write.csv(anntext(), file, row.names=FALSE)
}) #end of output$download.text
output$mytable1 = renderDataTable({
inFile <- input$file1
#if(is.null(inFile)) {return(NULL)}
#else{
table = anntext()
return(table)
#}
}) #end of output$mytable
output$nounwordcloud = renderPlot({
inFile = input$file1 #content in the uploaded file
color = input$radio #colors for the word cloud plots
nowords = input$integer1 #no of words to plot
size = input$integer2 #size of cooccurance plot
minfreq = input$integer3 #min freq of words to plot
range1 = input$range1 #range of words in the wordcloud
if(is.null(inFile)) {return(NULL)}
else{
nouns = anntext() %>% subset(., upos %in% "NOUN")
nounsdf = nouns %>% group_by(lemma) %>% count(lemma, sort=TRUE) #count number of nouns
colnames(nounsdf) = c("nouns", "count") #rename the columns
wordcloud(nounsdf$nouns, nounsdf$count, # words, their freqs
scale = range1, #range of words
min.freq=minfreq, # min.freq of words to consider
max.words = nowords, # max #words
colors = brewer.pal(6, color))
}
}) #end of output$noun.wordcloud
output$verbwordcloud = renderPlot({
inFile = input$file1 #content in the uploaded file
color = input$radio #colors for the word cloud plots
nowords = input$integer1 #no of words to plot
size = input$integer2 #size of cooccurance plot
minfreq = input$integer3 #min freq of words to plot
range1 = input$range1 #range of words in the wordcloud
#if(is.null(inFile)) {return(NULL)}
#else{
verbs = anntext() %>% subset(., upos %in% "VERB")
verbsdf = verbs %>% group_by(lemma) %>% count(lemma, sort=TRUE) #count nouns
colnames(verbsdf) = c("verbs", "count") #rename the columns
wordcloud(verbsdf$verbs, verbsdf$count, # words, their freqs
scale = range1, #range of words
min.freq=minfreq, # min.freq of words to consider
max.words = nowords, # max #words
colors = brewer.pal(6, color))
#} #end of else
}) #output$verb.wordcloud
output$anndoccooc = renderPlot({
inFile = input$file1 #content in the uploaded file
color = input$radio #colors for the word cloud plots
nowords = input$integer1 #no of words to plot
size = input$integer2 #size of cooccurance plot
minfreq = input$integer3 #min freq of words to plot
range1 = input$range1 #range of words in the wordcloud
#if(is.null(inFile)) {return(NULL)}
#else{
anndoccooc = cooccurrence(x = subset(anntext(), upos %in% input$checkGroup),
term = "lemma",
group = c("doc_id", "paragraph_id", "sentence_id"))
words = head(anndoccooc, nowords) #display a plot of top-30 co-occurrences
words = igraph::graph_from_data_frame(words) # creates an igraph object for plotting
ggraph(words, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "orange") +
geom_node_text(aes(label = name), col = "darkgreen", size = size) +
theme_graph(base_family = "Arial Narrow") +
theme(legend.position = "none")
#  } #end of else
}) #end of output$ann.doc.cooc
} #end of function
shinyApp(ui=ui, server=server)
display.brewer.all()
?pdf_tools
??pdf_tools
??pdf_text
??read_docx
require(XML)
require(tm)
require(wordcloud)
require(RColorBrewer)
u = "http://cran.r-project.org/web/packages/available_packages_by_date.html"
t = readHTMLTable(u)[[1]]
ap.corpus <- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
ap.corpus <- tm_map(ap.corpus, removePunctuation)
ap.corpus <- tm_map(ap.corpus, content_transformer(tolower))
ap.corpus <- tm_map(ap.corpus, function(x) removeWords(x, stopwords("english")))
ap.corpus <- Corpus(VectorSource(ap.corpus))
ap.tdm <- TermDocumentMatrix(ap.corpus)
ap.m <- as.matrix(ap.tdm)
ap.v <- sort(rowSums(ap.m),decreasing=TRUE)
ap.d <- data.frame(word = names(ap.v),freq=ap.v)
table(ap.d$freq)
pal2 <- brewer.pal(8,"Dark2")
png("wordcloud_packages.png", width=1280,height=800)
wordcloud(ap.d$word,ap.d$freq, scale=c(8,.2),min.freq=3,
max.words=Inf, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
ap.corpus <- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
u = "http://cran.r-project.org/web/packages/available_packages_by_date.html"
t = readHTMLTable(u)[[1]]
ap.corpus <- Corpus(DataframeSource(data.frame(as.character(t[,3]))))
??get_sentiments
install.packages("rsconnect")
rsconnect::setAccountInfo(name='vikramdevatha', token='69AAB1D72B531820D5B17D3A56FF4F6F', secret='kHkwmjHPyyr+Z7uWo8Sgi8SJ0OBB2g8AAWPLIiZo')
devtools::install.github('rstudio/shinyapps')
install.github('rstudio/shinyapps')
library("devtools", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
devtools::install.github('rstudio/shinyapps')
devtools::install_github('rstudio/shinyapps')
runApp()
if (!require(dplyr)) {install.packages("dplyr")}; library(dplyr)
if (!require(udpipe)) {install.packages("udpipe")}; library(udpipe)
if (!require(textrank)) {install.packages("textrank")}; library(textrank)
if (!require(lattice)) {install.packages("lattice")}; library(lattice)
if (!require(igraph)) {install.packages("igraph")}; library(igraph)
if (!require(ggraph)) {install.packages("ggraph")}; library(ggraph)
if (!require(ggplot2)) {install.packages("ggplot2")}; library(ggplot2)
if (!require(wordcloud)) {install.packages("wordcloud")}; library(wordcloud)
if (!require(RColorBrewer)) {install.packages("RColorBrewer")}; library(RColorBrewer)
if (!require(stringr)) {install.packages("stringr")}; library(stringr)
if (!require(textreadr)) {install.packages("textreadr")}; library(textreadr)
if (!require(tools)) {install.packages("tools")}; library(tools)
if (!require(shiny)) {install.packages("shiny")}; library(shiny)
if (!require(shinyjs)) {install.packages("shinyjs")}; library(shinyjs)
if (!require(pdftools)) {install.packages("pdftools")}; library(pdftools)
if (!require(tidytext)) {install.packages("tidytext")}; library(tidytext)
runApp()
getwd()
setwd('/Users/vikram/Documents/GitHub/text-an-app')
runApp()
deployApp()
??deployApp
library("rsconnect", lib.loc="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
deployApp()
runApp()
deployApp()
deployApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
source('https://raw.githubusercontent.com/vikramdevatha/CV-Analyser/master/dependencies.R')
runGitHub('CV-Analyser', 'vikramdevatha')
shiny::runApp()
shiny::runApp()
Dataset = "The 1988 Atlantic hurricane season was a moderately active season that proved costly and deadly, with 15 tropical cyclones directly affecting land. The first cyclone to attain tropical storm status was Alberto on August 8; the final storm of the year, Tropical Storm Keith, became extratropical on November 24. The season produced 19 tropical depressions, including 12 tropical storms. There were five hurricanes, including three major hurricanes (Category 3 or higher on the Saffir???Simpson scale). Hurricane Gilbert (pictured) was at the time the strongest Atlantic hurricane on record. It tracked through the Caribbean Sea and the Gulf of Mexico and caused devastation in Mexico and many island nations, particularly Jamaica. Its passage caused damage valued at US$2.98 billion and more than 300 deaths, mostly in Mexico. Hurricane Joan, striking Nicaragua as a Category 4 hurricane, caused damage valued at about $1.87 billion and more than 200 deaths. (Full article...)"
englishmodel = udpipe_load_model("english-ud-2.0-170801.udpipe")
anndoc = udpipe_annotate(englishmodel, Dataset) #tokenizes, tags and parses the text
anndoc = as.data.frame(anndoc) #converts the output into a data frame
anndoc = mutate(anndoc, sentence=NULL) #drop the sentence column from the data frame
head(anndoc, 100) #show only a hundred rows and store in a new variable
lexicon <- get_sentiments("afinn")
words = anndoc %>% select(token) %>%
anti_join(stop_words, by=c("token"="word")) %>%
left_join(lexicon, by = c("token"="word")) %>%
filter(!is.na(sentiment)) %>%
group_by(sentiment) %>%
summarise(score=n())
anndoc %>% select(token)
anndoc %>% select(token) %>%
anti_join(stop_words, by=c("token"="word"))
anndoc %>% select(token) %>%
anti_join(stop_words, by=c("token"="word")) %>%
left_join(lexicon, by = c("token"="word"))
words = anndoc %>% select(token) %>%
anti_join(stop_words, by=c("token"="word")) %>%
left_join(lexicon, by = c("token"="word")) %>%
filter(!is.na(score)) %>%
group_by(token) %>%
summarise(score=n())
words
words[order(-words$score),]
words[order(-words$score),]
shiny::runApp()
source('https://raw.githubusercontent.com/vikramdevatha/CV-Analyser/master/dependencies.R')
runGitHub('CV-Analyser', 'vikramdevatha')
source("https://raw.githubusercontent.com/sudhir-voleti/basic-textanalysis-shinyapp/master/dependency-basic-text-analysisshinyapp.R")
source("https://raw.githubusercontent.com/sudhir-voleti/tidysentiment-analysis/master/dependency-tidy-sentiment-analysis.R")
source("https://raw.githubusercontent.com/sudhir-voleti/basic-text-app")
source('https://github.com/sudhir-voleti/basic-text-app/blob/master/dependency-basic-text-analysis-shinyapp.R')
source('https://github.com/sudhir-voleti/basic-text-app/blob/master/dependency-basic-text-analysis-shinyapp.R')
runGitHub('basic-text-app', 'sudhir-voleti')
install.packages("qdap")
build_kmeans_scree <- function(mydata)  # rows are units, colms are basis variables
{# Determine number of clusters
set.seed(seed = 0000)   # set seed for reproducible work
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))  # wss is within group sum of squares
for (i in 2:15) wss[i] <- sum(      # checking model fit for 2 to 15 clusters
kmeans(mydata,  centers = i)$withinss)  # note use of kmeans() func
plot(1:15, wss, type="b",  # scree.plot =
xlab="Number of Clusters",
ylab="Within groups sum of squares")
# return(scree.plot)
}
library(tibble)
library(tidyr)
library(readr)
library(purr)
library(purr)
library(purrr)
ice_cream = readLines("/Users/vikram/Documents/Dropbox/Work/CV/Others/Mariana/Mariana Cortina 2018 without FORMAT.docx");    length(ice_cream)
source('https://raw.githubusercontent.com/vikramdevatha/textan/master/Text_Clean.R')
source('https://raw.githubusercontent.com/vikramdevatha/textan/master/DTM_Builder.R')
ice_cream %>%
Text_Clean() %>%
DTM_Builder()
dtm_ice_cream = ice_cream %>%
Text_Clean() %>%
DTM_Builder()
dim(dtm_ice_cream)
build_kmeans_scree(dtm_ice_cream)
ice_cream = readLines("/Users/vikram/Documents/Dropbox/Work/CV/20180406 - Vikram Devatha.docx");    length(ice_cream)
dtm_ice_cream = ice_cream %>%
Text_Clean() %>%
DTM_Builder()
dim(dtm_ice_cream)
build_kmeans_scree(dtm_ice_cream)
k=2
# K-Means Cluster Analysis
fit <- kmeans(dtm, k) # k cluster solution
dtm = ice_cream %>%
Text_Clean() %>%
DTM_Builder()
# K-Means Cluster Analysis
fit <- kmeans(dtm, k) # k cluster solution
fit
View(fit)
fit$cluster
display.clusters <- function(dtm, k)  # k=optimal num of clusters
{
# K-Means Cluster Analysis
fit <- kmeans(dtm, k) # k cluster solution
for (i1 in 1:max(fit$cluster)){
#   windows()
dtm_cluster = dtm[(fit$cluster == i1),]
distill.cog(dtm_cluster)    } # i1 loop ends
}
dtm_ice_cream = ice_cream %>%
Text_Clean() %>%
DTM_Builder()
# test driving the func
system.time({ display.clusters(dtm_ice_cream, 3) })
source('https://raw.githubusercontent.com/vikramdevatha/textan/master/COG.R')
display.clusters <- function(dtm, k)  # k=optimal num of clusters
{
# K-Means Cluster Analysis
fit <- kmeans(dtm, k) # k cluster solution
for (i1 in 1:max(fit$cluster)){
#   windows()
dtm_cluster = dtm[(fit$cluster == i1),]
COG(dtm_cluster)    } # i1 loop ends
}
# test driving the func
system.time({ display.clusters(dtm_ice_cream, 3) })
display.clusters <- function(dtm, k)  # k=optimal num of clusters
{
# K-Means Cluster Analysis
fit <- kmeans(dtm, k) # k cluster solution
for (i1 in 1:max(fit$cluster)){
#   windows()
dtm_cluster = dtm[(fit$cluster == i1),]
COG(dtm_cluster, central.nodes=2)    } # i1 loop ends
}
# test driving the func
system.time({ display.clusters(dtm_ice_cream, 3) })
# first convert dtm to an adjacency matrix i.e. 1 is there is a connection, 0 if there is not
dtm1 = as.matrix(dtm) # need regular matrix for matrix opertions
# first convert dtm to an adjacency matrix i.e. 1 is there is a connection, 0 if there is not
dtm1 = as.matrix(dtm) # need regular matrix for matrix opertions
adj.mat = t(dtm1) %*% dtm1 # making a square symmatric term-term matrix
diag(adj.mat) = 0 # no self-references. So diagonal is 0
a0 = order(apply(adj.mat, 2, sum), decreasing = T)   # order cols by descending colSum
mat1 = as.matrix(adj.mat[a0[1:50], a0[1:50]]) # take the top 50 items
a = colSums(mat1) # collect column sums into a vector obj a
b = order(-a) # order the vector in descending order
mat2 = mat1[b, b] # order both rows and columns along vector b
diag(mat2) =  0 # no self-references. So diagonal is 0.
wc = NULL
central.nodes=4
for (i1 in 1:central.nodes){
thresh1 = mat2[i1,][order(-mat2[i1, ])[max.connexns]]
mat2[i1, mat2[i1,] < thresh1] = 0
mat2[i1, mat2[i1,] > 0 ] = 1
word = names(mat2[i1, mat2[i1,] > 0])
mat2[(i1+1):nrow(mat2), match(word,colnames(mat2))] = 0
wc = c(wc, word) #appends word
} # i1 loop ends
max.connexns = 5
for (i1 in 1:central.nodes){
thresh1 = mat2[i1,][order(-mat2[i1, ])[max.connexns]]
mat2[i1, mat2[i1,] < thresh1] = 0
mat2[i1, mat2[i1,] > 0 ] = 1
word = names(mat2[i1, mat2[i1,] > 0])
mat2[(i1+1):nrow(mat2), match(word,colnames(mat2))] = 0
wc = c(wc, word) #appends word
} # i1 loop ends
mat3 = mat2[match(wc, colnames(mat2)), match(wc, colnames(mat2))]
ord = colnames(mat2)[which(!is.na(match(colnames(mat2), colnames(mat3))))]  # removed any NAs from the list
mat4 = mat3[match(ord, colnames(mat3)), match(ord, colnames(mat3))]
# build and plot a network object
graph <- graph.adjacency(mat4, mode = "undirected", weighted=T) # Create Network object
graph = simplify(graph)
V(graph)$color[1:central.nodes] = "green"
graph = simplify(graph)
graph = delete.vertices(graph, V(graph)[ degree(graph) == 0 ]) # delete singles
plot(graph,
layout = layout.kamada.kawai,
main = title)
plot(graph)
plot(graph),
layout = layout.kamada.kawai,
main = title)
plot(graph,
layout = layout.kamada.kawai,
main = title)
k=NULL
plot(graph,
layout = layout.kamada.kawai,
main = title)
K=2
plot(graph,
layout = layout.kamada.kawai,
main = title)
COG(dtm)
